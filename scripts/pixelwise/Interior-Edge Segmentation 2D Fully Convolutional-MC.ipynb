{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Interior_Edge_Segmentation_2D_Fully_Convolutional.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uMZIGjTbs-B",
        "colab_type": "text"
      },
      "source": [
        "# Fully Convolutional Interior/Edge Segmentation for 2D Data\n",
        "\n",
        "---\n",
        "\n",
        "Classifies each pixel as either Cell Edge, Cell Interior, or Background.\n",
        "\n",
        "There are 2 different Cell Edge classes (Cell-Cell Boundary and Cell-Background Boundary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xhffYv5f0cq",
        "colab_type": "code",
        "outputId": "84f9782b-9df9-4905-dad0-aac5429bb987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/TFG/TFG MARINA CALZADA/Repo Github"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/TFG/TFG MARINA CALZADA/Repo Github\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MsXqylCZ52E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/aniramcg/deepcell-tf.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-QZMZnprmO8",
        "colab_type": "code",
        "outputId": "d9b321ae-bd6d-480d-fffc-6f46f1979734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/TFG/TFG MARINA CALZADA/Repo Github/deepcell-tf')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build\t\t    deepcell\tLICENSE     requirements-test.txt  setup.py\n",
            "CODE_OF_CONDUCT.md  Dockerfile\tpytest.ini  requirements.txt\n",
            "CONTRIBUTING.md     docs\tREADME.md   scripts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLTsklwIsi5m",
        "colab_type": "code",
        "outputId": "6ee4562d-1bd2-4d83-d523-98b636b60888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python setup.py build_ext --inplace\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "skipping 'deepcell/utils/compute_overlap.c' Cython extension (up-to-date)\n",
            "copying build/lib.linux-x86_64-3.6/deepcell/utils/compute_overlap.cpython-36m-x86_64-linux-gnu.so -> deepcell/utils\n",
            "Requirement already satisfied: pandas<1,>=0.23.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.25.3)\n",
            "Requirement already satisfied: numpy<2,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: scipy<2,>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: scikit-image<1,>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.16.2)\n",
            "Requirement already satisfied: scikit-learn<1,>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow<2,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.15.2)\n",
            "Requirement already satisfied: jupyter<2,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: nbformat<5,>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (4.4.0)\n",
            "Requirement already satisfied: keras-applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.0.8)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (2.4)\n",
            "Requirement already satisfied: opencv-python<4,>=3.4.2.17 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (3.4.9.33)\n",
            "Requirement already satisfied: cython>=0.28 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.29.17)\n",
            "Requirement already satisfied: pathlib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: deepcell-tracking>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: deepcell-toolbox>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (0.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<1,>=0.23.3->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas<1,>=0.23.3->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<1,>=0.14.1->-r requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<1,>=0.14.1->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<1,>=0.14.1->-r requirements.txt (line 4)) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<1,>=0.14.1->-r requirements.txt (line 4)) (7.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<1,>=0.19.1->-r requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (0.34.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.28.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (4.7.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (7.5.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat<5,>=4.4.0->-r requirements.txt (line 8)) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat<5,>=4.4.0->-r requirements.txt (line 8)) (4.6.3)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat<5,>=4.4.0->-r requirements.txt (line 8)) (4.3.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat<5,>=4.4.0->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications==1.0.8->-r requirements.txt (line 9)) (2.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->-r requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: keras-retinanet in /usr/local/lib/python3.6/dist-packages (from deepcell-toolbox>=0.3.0->-r requirements.txt (line 15)) (0.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<1,>=0.14.1->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<1,>=0.14.1->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<1,>=0.14.1->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2,>=1.14.0->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (2.1.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (3.1.5)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (2.11.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (5.3.4)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (4.5.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (1.0.18)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (19.0.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (1.9.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.8.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (3.5.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->deepcell-toolbox>=0.3.0->-r requirements.txt (line 15)) (2.3.1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->deepcell-toolbox>=0.3.0->-r requirements.txt (line 15)) (3.38.0)\n",
            "Requirement already satisfied: keras-resnet in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->deepcell-toolbox>=0.3.0->-r requirements.txt (line 15)) (0.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (20.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (1.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.1.9)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter<2,>=1.0.0->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet->deepcell-toolbox>=0.3.0->-r requirements.txt (line 15)) (3.13)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet->deepcell-toolbox>=0.3.0->-r requirements.txt (line 15)) (2.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiYMeRUBbs-I",
        "colab_type": "code",
        "outputId": "45cd8cbc-c0fd-4aa3-810d-2f63ab966045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import os\n",
        "import errno\n",
        "\n",
        "import numpy as np\n",
        "!pip install SimpleITK\n",
        "import deepcell"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.6/dist-packages (1.2.4)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKLVGqtibs-R",
        "colab_type": "text"
      },
      "source": [
        "### Load the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtHn4ufH9tCM",
        "colab_type": "text"
      },
      "source": [
        "#### Use this command if you are working in Google Colab to avoid RAM out of memory problems\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwoujzRg2Hu_",
        "colab_type": "code",
        "outputId": "17ae1c1c-062a-4964-8126-c5127b77447e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install SimpleITK\n",
        "import SimpleITK as sitk\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def load_itk_image(filename):\n",
        "#     itkimage = sitk.ReadImage(filename)\n",
        "#     numpyImage = sitk.GetArrayFromImage(itkimage)\n",
        "#     return numpyImage\n",
        "\n",
        "path2data = '/content/drive/My Drive/TFG/TFG MARINA CALZADA/clean_data/deepcell/data.npz'\n",
        "training_data = np.load(path2data)\n",
        "# X_train = training_data['x_train']\n",
        "X_test = training_data['x_test']\n",
        "# y_train = training_data['y_train']\n",
        "y_test = training_data['y_test']\n",
        "print(X_test.shape[-1])\n",
        "\n",
        "# plt.figure()\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.imshow(X_train[89,:,:,0])\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.imshow(y_test[89,:,:,0])\n",
        "# plt.show()\n",
        "# print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))\n",
        "# print('Xtest.shape: {}\\ny.shape: {}'.format(X_test.shape, y_test.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.6/dist-packages (1.2.4)\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ZAr4AxqG4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path2data = '/content/drive/My Drive/TFG/TFG MARINA CALZADA/clean_data/deepcell/data.npz'\n",
        "test_size = 0.1 # % of data saved as test\n",
        "seed = 0 # seed for random train-test split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qahxIzrLvNQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_size = 0.1 # % of data saved as test\n",
        "# seed = 0 # seed for random train-test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# training_data = np.load('/content/drive/My Drive/Repo Github/example.npz')\n",
        "# X = training_data['X']\n",
        "# y = training_data['y']\n",
        "# print(y.shape)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X, y, test_size=test_size, random_state=seed)\n",
        "# del X, y\n",
        "# print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))\n",
        "# plt.figure()\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.imshow(X_test[1,:,:,0])\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.imshow(y_test[1,:,:,0])\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuPa2fLv9e5-",
        "colab_type": "text"
      },
      "source": [
        "#### Use the next command if you are running the model in your local computer as google colab crashes completely due to the large size of the data HELA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOL_n5sYbs-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Download the data (saves to ~/.keras/datasets)\n",
        "# filename = 'HeLa_S3.npz'\n",
        "# test_size = 0.2 # % of data saved as test\n",
        "# seed = 0 # seed for random train-test split\n",
        "\n",
        "# # NOTE: pass the same seed and test_size to load_data and to train_model_conv to train on the same train-test split as the one in the line below\n",
        "# (X_train, y_train), (X_test, y_test) = deepcell.datasets.hela_s3.load_data(filename, test_size=test_size, seed=seed)\n",
        "\n",
        "# print('X.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5811SUUbs-Y",
        "colab_type": "text"
      },
      "source": [
        "### Set up filepath constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW0E0S14bs-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the path to the data file is currently required for `train_model_()` functions\n",
        "\n",
        "# NOTE: Change DATA_DIR if you are not using `deepcell.datasets`\n",
        "# DATA_DIR = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
        "DATA_DIR = '/content/drive/My Drive/TFG/TFG MARINA CALZADA/clean_data/deepcell/'\n",
        "DATA_FILE = '/content/drive/My Drive/TFG/TFG MARINA CALZADA/clean_data/deepcell/'\n",
        "# filename = 'data.npz'\n",
        "# DATA_FILE = os.path.join(DATA_DIR, filename)\n",
        "\n",
        "# confirm the data file is available\n",
        "# assert os.path.isfile(DATA_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJteyBTxbs-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up other required filepaths\n",
        "\n",
        "# If the data file is in a subdirectory, mirror it in MODEL_DIR and LOG_DIR\n",
        "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
        "\n",
        "# ROOT_DIR = '/data'  # TODO: Change this! Usually a mounted volume\n",
        "ROOT_DIR = '/content/drive/My Drive/Repo Github'\n",
        "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
        "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))\n",
        "\n",
        "# create directories if they do not exist\n",
        "for d in (MODEL_DIR, LOG_DIR):\n",
        "    try:\n",
        "        os.makedirs(d)\n",
        "    except OSError as exc:  # Guard against race condition\n",
        "        if exc.errno != errno.EEXIST:\n",
        "            raise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZL2Xi94bs-k",
        "colab_type": "text"
      },
      "source": [
        "### Set up training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jdwYwD6bs-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from deepcell.utils.train_utils import rate_scheduler\n",
        "\n",
        "fgbg_model_name = 'conv_fgbg_model'\n",
        "conv_model_name = 'conv_edgeseg_model'\n",
        "\n",
        "n_epoch = 3  # Number of training epochs\n",
        "norm_method = 'std'  # data normalization\n",
        "receptive_field = 61  # should be adjusted for the scale of the data\n",
        "\n",
        "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
        "\n",
        "# FC training settings\n",
        "n_skips = 3  # number of skip-connections (only for FC training)\n",
        "batch_size = 1  # FC training uses 1 image per batch\n",
        "\n",
        "# Transformation settings\n",
        "transform = 'pixelwise'\n",
        "dilation_radius = 1  # change dilation radius for edge dilation\n",
        "separate_edge_classes = True  # break edges into cell-background edge, cell-cell edge\n",
        "n_features = 4 if separate_edge_classes else 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCWe-WVebs-q",
        "colab_type": "text"
      },
      "source": [
        "### First, create a foreground/background separation model\n",
        "\n",
        "#### Instantiate the fgbg model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY2UVrh_bs-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from deepcell import model_zoo\n",
        "\n",
        "fgbg_model = model_zoo.bn_feature_net_skip_2D(\n",
        "    n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
        "    receptive_field=receptive_field,\n",
        "    n_skips=n_skips,\n",
        "    n_conv_filters=32,\n",
        "    n_dense_filters=128,\n",
        "    input_shape = tuple((None, None, 1)),\n",
        "    # input_shape=tuple((256,256,1)), # indicar el shape de las imágenes de entrenamiento\n",
        "    last_only=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JsOwSQRbs-w",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model fgbg model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XluzwnpRbs-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from deepcell.training import train_model_conv\n",
        "\n",
        "# fgbg_model = train_model_conv(\n",
        "#     model=fgbg_model,\n",
        "#     dataset=DATA_FILE,  # full path to npz file\n",
        "#     model_name=fgbg_model_name,\n",
        "#     # test_size=test_size,\n",
        "#     seed=seed,\n",
        "#     optimizer=optimizer,\n",
        "#     n_epoch=n_epoch,\n",
        "#     batch_size=batch_size,\n",
        "#     transform='fgbg',\n",
        "#     model_dir=MODEL_DIR,\n",
        "#     log_dir=LOG_DIR,\n",
        "#     lr_sched=lr_sched,\n",
        "#     rotation_range=180,\n",
        "#     flip=True,\n",
        "#     shear=False,\n",
        "#     zoom_range=(0.8, 1.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UHTb8P-9VdQ",
        "colab_type": "text"
      },
      "source": [
        "#### Load the trained fgbg model from the MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GkH8itX8JNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fgbg_model.load_weights(os.path.join(MODEL_DIR, 'fgbg_model' + '.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LypYyFoDbs-2",
        "colab_type": "text"
      },
      "source": [
        "### Next, Create a model for the edge/interior segmentation\n",
        "\n",
        "#### Instantiate the segmentation transform model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOgjg2vqbs-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from deepcell import model_zoo\n",
        "\n",
        "# conv_model = model_zoo.bn_feature_net_skip_2D(\n",
        "#     fgbg_model=fgbg_model,\n",
        "#     receptive_field=receptive_field,\n",
        "#     n_skips=n_skips,\n",
        "#     n_features=n_features,\n",
        "#     norm_method=norm_method,\n",
        "#     n_conv_filters=32,\n",
        "#     n_dense_filters=128,\n",
        "#     last_only=False,\n",
        "#     # input_shape=tuple(X_train.shape[1:]))\n",
        "#     input_shape=tuple((256,256,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTH4KBWkbs-8",
        "colab_type": "text"
      },
      "source": [
        "#### Train the segmentation transform model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKC38h4Lbs-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from deepcell.training import train_model_conv\n",
        "\n",
        "# conv_model = train_model_conv(\n",
        "#     model=conv_model,\n",
        "#     dataset=DATA_FILE,  # full path to npz file\n",
        "#     model_name=conv_model_name,\n",
        "#     test_size=test_size,\n",
        "#     seed=seed,\n",
        "#     transform=transform,\n",
        "#     dilation_radius=dilation_radius,\n",
        "#     separate_edge_classes=separate_edge_classes,\n",
        "#     optimizer=optimizer,\n",
        "#     batch_size=batch_size,\n",
        "#     n_epoch=n_epoch,\n",
        "#     log_dir=LOG_DIR,\n",
        "#     model_dir=MODEL_DIR,\n",
        "#     lr_sched=lr_sched,\n",
        "#     rotation_range=180,\n",
        "#     flip=True,\n",
        "#     shear=False,\n",
        "#     zoom_range=(0.8, 1.2),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411nVyLAbs_C",
        "colab_type": "text"
      },
      "source": [
        "### Run the model\n",
        "\n",
        "#### Make predictions on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO1E_WaBbs_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_images = conv_model.predict(X_test)[-1]\n",
        "test_images_fgbg = fgbg_model.predict(X_test)[-1]\n",
        "# test_images_fgbg = fgbg_model.evaluate(X_test,batch_size=1,verbose=1)\n",
        "\n",
        "# print('watershed transform shape:', test_images.shape)\n",
        "print('segmentation mask shape:', test_images_fgbg.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0B_QzEjbs_K",
        "colab_type": "text"
      },
      "source": [
        "#### Post-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7G-ttVGbs_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# threshold the foreground/background\n",
        "# and remove back ground from edge transform\n",
        "threshold = 0.9\n",
        "\n",
        "fg_thresh = test_images_fgbg[..., 1] > threshold\n",
        "fg_thresh = np.expand_dims(fg_thresh, axis=-1)\n",
        "\n",
        "test_images_post_fgbg = test_images * fg_thresh\n",
        "test_images_post_fgbg = fg_thresh\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQzTG07mbs_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label interior predictions\n",
        "from skimage.measure import label\n",
        "from skimage import morphology\n",
        "\n",
        "labeled_images = []\n",
        "for i in range(test_images_post_fgbg.shape[0]):\n",
        "    interior = test_images_post_fgbg[i, ..., 2] > .2\n",
        "    labeled_image = label(interior)\n",
        "    labeled_image = morphology.remove_small_objects(\n",
        "        labeled_image, min_size=50, connectivity=1)\n",
        "    labeled_images.append(labeled_image)\n",
        "labeled_images = np.array(labeled_images)\n",
        "labeled_images = np.expand_dims(labeled_images, axis=-1)\n",
        "\n",
        "print('labeled_images shape:', labeled_images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm69WqG-bs_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "index = np.random.randint(low=0, high=X_test.shape[0])\n",
        "print('Image number:', index)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(15, 15), sharex=True, sharey=True)\n",
        "ax = axes.ravel()\n",
        "\n",
        "ax[0].imshow(X_test[index, ..., 0])\n",
        "ax[0].set_title('Source Image')\n",
        "\n",
        "ax[1].imshow(test_images_fgbg[index, ..., 1])\n",
        "ax[1].set_title('Segmentation Prediction')\n",
        "\n",
        "ax[2].imshow(fg_thresh[index, ..., 0], cmap='jet')\n",
        "ax[2].set_title('FGBG Threshold {}%'.format(threshold * 100))\n",
        "\n",
        "ax[3].imshow(test_images[index, ..., 0] + test_images[index, ..., 1], cmap='jet')\n",
        "ax[3].set_title('Edge Prediction')\n",
        "\n",
        "ax[4].imshow(test_images[index, ..., 2], cmap='jet')\n",
        "ax[4].set_title('Interior Prediction')\n",
        "\n",
        "ax[5].imshow(labeled_images[index, ..., 0], cmap='jet')\n",
        "ax[5].set_title('Instance Segmentation')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze4u-3TNkepT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_19Qi3_ybs_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "\n",
        "# This function is placed in deepcell.utils.data_utils\n",
        "# Randomly crop the image to a specific size. For data augmentation\n",
        "def random_crop(image, label, crop_height, crop_width):\n",
        "    if (image.shape[0] != label.shape[0]) or (image.shape[1] != label.shape[1]):\n",
        "        raise Exception('Image and label must have the same dimensions!')\n",
        "    if (crop_width <= image.shape[1]) and (crop_height <= image.shape[0]):\n",
        "        pdf_im = np.ones(label.shape) # y is a mask\n",
        "        pdf_im[label>0]=10000 # pdf aquí es un peso. Por ejemplo 10000. \n",
        "        # cropw = int(crop_width/2)\n",
        "        # croph = int(crop_height/2)\n",
        "        pdf_im = pdf_im[:-crop_height,:-crop_width] # limit the coordinates in which a centroid can lay\n",
        "        prob = np.float32(pdf_im)\n",
        "        prob = prob.ravel()/np.sum(prob) # convert the 2D matrix into a vector and normalize it so you create a distribution of all the possible values between 1 and prod(pdf.shape)(sum=1)\n",
        "        choices = np.prod(pdf_im.shape) \n",
        "        index = np.random.choice(choices, size=1,p = prob) # get a random centroid but following a pdf distribution.\n",
        "        coordinates = np.unravel_index(index, shape=pdf_im.shape)\n",
        "        y = coordinates[0][0]\n",
        "        x = coordinates[1][0]\n",
        "        return image[y:y+crop_height, x:x+crop_width], label[y:y+crop_height, x:x+crop_width]\n",
        "    else:\n",
        "        raise Exception('Crop shape (%d, %d) exceeds image dimensions (%d, %d)!' % (crop_height, crop_width, image.shape[0], image.shape[1]))\n",
        "\n",
        "\n",
        "def get_data_MARINA(DATAPATH, mode='sample', test_size=.2, seed=0, crop_height=256, crop_width=256):\n",
        "    \"\"\"Load data from NPZ file and split into train and test sets\n",
        "\n",
        "    Args:\n",
        "        file_name (str): path to NPZ file to load\n",
        "        mode (str): if 'siamese_daughters', returns lineage information from\n",
        "            .trk file otherwise, returns the same data that was loaded.\n",
        "        test_size (float): percent of data to leave as testing holdout\n",
        "        seed (int): seed number for random train/test split repeatability\n",
        "\n",
        "    Returns:\n",
        "        (dict, dict): dict of training data, and a dict of testing data\n",
        "    \"\"\"\n",
        "    # siamese_daughters mode is used to import lineage data\n",
        "    # and associate it with the appropriate batch\n",
        "    # if mode == 'siamese_daughters':\n",
        "    #     training_data = load_trks(file_name)\n",
        "    #     X = training_data['X']\n",
        "    #     y = training_data['y']\n",
        "    #     # `daughters` is of the form:\n",
        "    #     #\n",
        "    #     #                   2 children / cell (potentially empty)\n",
        "    #     #                          ___________|__________\n",
        "    #     #                         /                      \\\n",
        "    #     #      daughers = [{id_1: [daughter_1, daughter_2], ...}, ]\n",
        "    #     #                  \\___________________________________/\n",
        "    #     #                                    |\n",
        "    #     #                       dict of (cell_id -> children)\n",
        "    #     #\n",
        "    #     # each batch has a separate (cell_id -> children) dict\n",
        "    #     daughters = [{cell: fields['daughters']\n",
        "    #                   for cell, fields in tracks.items()}\n",
        "    #                  for tracks in training_data['lineages']]\n",
        "\n",
        "    #     X_train, X_test, y_train, y_test, ln_train, ln_test = train_test_split(\n",
        "    #         X, y, daughters, test_size=test_size, random_state=seed)\n",
        "\n",
        "    #     train_dict = {\n",
        "    #         'X': X_train,\n",
        "    #         'y': y_train,\n",
        "    #         'daughters': ln_train\n",
        "    #     }\n",
        "\n",
        "    #     test_dict = {\n",
        "    #         'X': X_test,\n",
        "    #         'y': y_test,\n",
        "    #         'daughters': ln_test\n",
        "    #     }\n",
        "    #     return train_dict, test_dict\n",
        "\n",
        "    train_files = os.listdir(os.path.join(DATAPATH, 'train'))\n",
        "    X_train = None\n",
        "\n",
        "    for f in train_files:\n",
        "      input_im = sitk.ReadImage(os.path.join(DATAPATH, 'train',f))\n",
        "      input_im = sitk.GetArrayFromImage(input_im)\n",
        "      input_im = input_im[:,:,0]\n",
        "      mask_im = sitk.ReadImage(os.path.join(DATAPATH, 'train_labels',f))\n",
        "      mask_im = sitk.GetArrayFromImage(mask_im)\n",
        "      mask_im = mask_im[:,:,0]\n",
        "      mask_im[mask_im > 0] = 1\n",
        "      input_im, mask_im = random_crop(input_im, mask_im, crop_height, crop_width)      \n",
        "      input_im = input_im.reshape((1, crop_height, crop_width, 1))\n",
        "      mask_im = mask_im.reshape((1, crop_height, crop_width, 1))\n",
        "      if X_train is None: \n",
        "        X_train = input_im\n",
        "        y_train = mask_im\n",
        "      else:\n",
        "        X_train = np.concatenate((X_train,input_im), axis=1)\n",
        "        y_train = np.concatenate((y_train,mask_im), axis=1)\n",
        "\n",
        "\n",
        "      train_files = os.listdir(os.path.join(DATAPATH, 'test'))\n",
        "\n",
        "    X_test = None\n",
        "    for f in train_files:\n",
        "      input_im = sitk.ReadImage(os.path.join(DATAPATH, 'test',f))\n",
        "      input_im = sitk.GetArrayFromImage(input_im)\n",
        "      input_im = input_im[:,:,0]\n",
        "      mask_im = sitk.ReadImage(os.path.join(DATAPATH, 'test_labels',f))\n",
        "      mask_im = sitk.GetArrayFromImage(mask_im)\n",
        "      mask_im = mask_im[:,:,0]\n",
        "      mask_im[mask_im > 0] = 1\n",
        "      input_im, mask_im = random_crop(input_im, mask_im, crop_height, crop_width)      \n",
        "      input_im = input_im.reshape((1, crop_height, crop_width, 1))\n",
        "      mask_im = mask_im.reshape((1, crop_height, crop_width, 1))\n",
        "      if X_test is None: \n",
        "        X_test = input_im\n",
        "        y_test = mask_im\n",
        "      else:\n",
        "        X_test = np.concatenate((X_test,input_im), axis=1)\n",
        "        y_test = np.concatenate((y_train,mask_im), axis=1)\n",
        "\n",
        "    train_dict = {\n",
        "        'X': X_train,\n",
        "        'y': y_train\n",
        "    }\n",
        "\n",
        "    test_dict = {\n",
        "        'X': X_test,\n",
        "        'y': y_test\n",
        "    }\n",
        "\n",
        "    return train_dict, test_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}